{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search import search_library\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI, OpenAIError\n",
    "import os\n",
    "\n",
    "def ask(sys_msg, usr_msg):\n",
    "    resp = client.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sys_msg},\n",
    "            {\"role\": \"user\", \"content\": usr_msg}\n",
    "        ]\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "def sample_text(text, x):\n",
    "    max_start = max(0, len(text) - x)\n",
    "    random_start = random.randint(0, max_start)\n",
    "    excerpt = text[random_start:random_start + x]\n",
    "    return excerpt\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"o4-mini-2025-04-16\")\n",
    "DB_FILE = \"library.sqlite\"\n",
    "MAX_RESULTS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s the minimal “data recipe” you’ll need in order to estimate your logit( success ) model and test  \n",
      "βforce_deployment > βverbal_threat.  You can think of each row in your data frame as “one U.S. action toward one opponent over one stated foreign-policy goal.”\n",
      "\n",
      "1.  Unit of analysis  \n",
      "    •  action_id (unique)  \n",
      "    •  date (or at least year)  \n",
      "    •  opponent_state_id  \n",
      "    •  goal_id  \n",
      "\n",
      "2.  Dependent variable  \n",
      "    •  success  (0/1)  \n",
      "       –  Definition: whether the U.S. achieved its stated objective vis-à-vis that opponent on that goal.  \n",
      "       –  Possible sources: post-action diplomatic summaries, news‐coding (e.g. GDELT, ICEWS), archival/family dispatches (DDI), secondary scholarly codings.  \n",
      "\n",
      "3.  Treatment variable  \n",
      "    •  us_action  (categorical)  \n",
      "       –  Must include at least:  \n",
      "         ­ force_deployment  (border incursions, limited no-fly zones, show-of-force deployments)  \n",
      "         ­ verbal_threat     (public threats, ultimatums, warnings, diplomatic démarches absent troop movements)  \n",
      "       –  (Later you’ll dummy-encode C(us_action) and test βforce_deployment>βverbal_threat.)  \n",
      "       –  Source: event‐data projects (CAMEO, ICEWS, GDELT) or hand‐coded from DDI, Presidential Daily Transcripts, State Department releases.  \n",
      "\n",
      "4.  Core controls (all at the time of the action)  \n",
      "    –  opponent_gdp_pc         (e.g. World Bank WDI)  \n",
      "    –  opponent_mil_spend_pc   (e.g. SIPRI military spending data ÷ population)  \n",
      "    –  opponent_allies         (number of formal defense pacts—ATOP or Correlates of War data)  \n",
      "    –  us_ally_small           (0/1: does the U.S. have at least one “secondary” ally in that region?)  \n",
      "    –  us_ally_great           (0/1: does the U.S. have at least one “major‐power” ally vis-à-vis that opponent?)  \n",
      "    –  presidential_approval   (e.g. Gallup monthly approval at nearest date)  \n",
      "    –  election_year           (0/1: U.S. presidential election year)  \n",
      "    –  docs_statements_consistency  \n",
      "         •  A numeric score (e.g. 0–1) of how consistent U.S. public/government statements were in the run-up to the action.  \n",
      "         •  (Can be hand‐coded from State/Presidential statements; COS-P consistency index; inter-coder reliability.)  \n",
      "    –  forces_pre_log          (log of U.S. combat‐capable personnel/resources pre-positioned near that opponent)  \n",
      "         •  E.g. pre‐action troop levels from DOD reports or press releases  \n",
      "\n",
      "5.  Fixed effects (to soak up unobserved heterogeneity)  \n",
      "    –  opponent_fixed_effect  = C(opponent_state_id)  \n",
      "    –  goal_fixed_effect      = C(goal_id)  \n",
      "\n",
      "6.  Clustering  \n",
      "    –  cluster on opponent_state_id  \n",
      "\n",
      "7.  Potential auxiliary variables (for diagnostics or robustness)  \n",
      "    •  regional_conflict_intensity (e.g. number of ongoing armed conflicts nearby)—to check omitted confounders  \n",
      "    •  domestic_polarization (Senate/Congress polarization index)  \n",
      "    •  year (linear or splines) to pick up time trends  \n",
      "\n",
      "8.  Data sources at a glance  \n",
      "    –  Event coding:  GDELT, ICEWS, CAMEO, or your own hand‐coding of US “actions” via State/Presidential archives  \n",
      "    –  Economic/military controls:  World Bank WDI, SIPRI, Correlates of War, ATOP  \n",
      "    –  Approval/elections:  Gallup, Roper, FEARS dataset  \n",
      "    –  Deployment levels:  DOD press releases, Military Balance (IISS), DoD’s Base Structure Report  \n",
      "    –  Statements consistency:  textual coding of press briefings, Presidential speeches, State cables  \n",
      "\n",
      "Once you assemble a longitudinal “action‐level” .csv or DataFrame with all of the above, you can run your Statsmodels logit with clustered SE, estimate βs for each us_action category, and then test the contrast βforce_deployment>βverbal_threat.\n"
     ]
    }
   ],
   "source": [
    "h1 = \"\"\"1.  Effectiveness of Different Action Types  \n",
    "   Hypothesis 1.1 (“force vs. non‐force”):  After controlling for opponent capability, allies, domestic politics, etc., deployment of actual forces (border incursions, no-fly zones) yields a higher probability of success than purely verbal actions (direct diplomacy or threats).  \n",
    "   – Empirical test:  Include dummies for each action type in a pooled logit; test pairwise contrasts (e.g., β_force_deploy > β_verbal_threat).\"\"\"\n",
    "p1 = \"\"\"### Hypothesis 1.1: force_vs_nonforce  \n",
    "• Variables  \n",
    "  – Treatment(s): C(us_action) with “force_deployment” vs “verbal_threat”  \n",
    "  – Outcome: success  \n",
    "  – Key control(s): opponent_gdp_pc, opponent_mil_spend_pc, opponent_allies, us_ally_small, us_ally_great, presidential_approval, election_year, docs_statements_consistency, forces_pre_log  \n",
    "• Model  \n",
    "  – Equation: logit(success) = β₀+β₁·I(force_deployment)+β₂·I(verbal_threat)+…+α_opponent+γ_goal  \n",
    "  – Estimation: logistic regression; opponent_fixed_effect & goal_fixed_effect; SE clustered by opponent_state  \n",
    "• Diagnostic checks  \n",
    "  – Pairwise contrast β₁>β₂; VIF for multicollinearity; Hosmer–Lemeshow  \n",
    "• Code skeleton (`statsmodels` Python)  \n",
    "```python\n",
    "import statsmodels.formula.api as smf\n",
    "model = smf.logit(\n",
    "    \"success ~ C(us_action) + opponent_gdp_pc + opponent_mil_spend_pc + \"\n",
    "    \"opponent_allies + us_ally_small + us_ally_great + presidential_approval + \"\n",
    "    \"election_year + docs_statements_consistency + forces_pre_log + \"\n",
    "    \"C(opponent_fixed_effect) + C(goal_fixed_effect)\",\n",
    "    data=df\n",
    ").fit(cov_type=\"cluster\", cov_kwds={\"groups\": df.opponent_state})\n",
    "```\"\"\"\n",
    "\n",
    "data = ask(\"What data do I need to collect to test the following hypothesis with the following implementation plan?\", f\"Hypothesis:\\n{h1}\\n\\nPlan:\\n{p1}\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bulldoze the globe for the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (crayon)",
   "language": "python",
   "name": "crayon"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
